{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors #\n",
    "\n",
    "A machine learning algorithm that classifies a data point using the labels of the $k$ nearest data points (neighbors). This is commonly handled with majority vote.\n",
    "\n",
    "Generally Euclidean distance can be used as the \"nearness\" measure, though for discrete variables other metrics like Hamming distance are used.\n",
    "\n",
    "The Euclidean distance between two data points $X$ and $Y$ each with $n$ features is:\n",
    "\n",
    "<center>$d(X, Y) = \\sqrt{(x_1 - y_1)^2 + \\dotsc + (x_n - y_n)^2}$</center>\n",
    "\n",
    "This can also be extended to a weighted classification system by weighing the vote of each neighbor by its nearness to the data point. From this standpoint, kNN is a weighting of 1/k to all k nearest neighbors and weight of 0 for the other data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "from sklearn import datasets, model_selection, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Data ###\n",
    "\n",
    "Import built-in iris data set and split it into a training and test set. Since there is some parameter tuning needed for setting $k$, I'll also create a validation set.\n",
    "\n",
    "The validation set will be created to have 20% of the data points from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X, Y, train_size=0.7)\n",
    "X_train,X_val,Y_train,Y_val = model_selection.train_test_split(X_train, Y_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Model ###\n",
    "\n",
    "Test different values for $k$ to find the \"optimal\" number of neighbors to use for classification. The scikit-learn implementation defaults the number of neighbors to 5 and odd numbers can be useful for breaking ties in the majority voting implementation. In the training set there are 84 data points.\n",
    "\n",
    "Therefore, for illustrative purposes and analysis I will test all odd numbers from 3 to 85. The latter of which is the same as using essentially all the data points for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 : 0.9523809523809523\n",
      "5 : 0.9523809523809523\n",
      "7 : 0.9523809523809523\n",
      "9 : 0.9523809523809523\n",
      "11 : 0.9523809523809523\n",
      "13 : 0.9523809523809523\n",
      "15 : 0.9523809523809523\n",
      "17 : 0.9523809523809523\n",
      "19 : 0.9523809523809523\n",
      "21 : 0.9523809523809523\n",
      "23 : 1.0\n",
      "25 : 1.0\n",
      "27 : 1.0\n",
      "29 : 0.9523809523809523\n",
      "31 : 0.9523809523809523\n",
      "33 : 0.9047619047619048\n",
      "35 : 0.9047619047619048\n",
      "37 : 0.9047619047619048\n",
      "39 : 0.9047619047619048\n",
      "41 : 0.9047619047619048\n",
      "43 : 0.9047619047619048\n",
      "45 : 0.9047619047619048\n",
      "47 : 0.9047619047619048\n",
      "49 : 0.9047619047619048\n",
      "51 : 0.9047619047619048\n",
      "53 : 0.9047619047619048\n",
      "55 : 0.9047619047619048\n",
      "57 : 0.38095238095238093\n",
      "59 : 0.3333333333333333\n",
      "61 : 0.3333333333333333\n",
      "63 : 0.3333333333333333\n",
      "65 : 0.3333333333333333\n",
      "67 : 0.3333333333333333\n",
      "69 : 0.3333333333333333\n",
      "71 : 0.3333333333333333\n",
      "73 : 0.3333333333333333\n",
      "75 : 0.3333333333333333\n",
      "77 : 0.3333333333333333\n",
      "79 : 0.3333333333333333\n",
      "81 : 0.3333333333333333\n",
      "83 : 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "k_vals = [i for i in range(3, 85, 2)]\n",
    "\n",
    "for k in k_vals:\n",
    "    model = knn(n_neighbors=k, weights='uniform')\n",
    "    model.fit(X_train,Y_train)\n",
    "    acc = model.score(X_val, Y_val)\n",
    "    print(k, ':', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis #####\n",
    "\n",
    "From the results of the accuracy above, we can see two things. The first is that effectively using any value between 3 and 31 results in a very high classifier accuracy on the validation set. We also see that as the setting for $k$ gets higher, the accuracy gets worse. This makes sense since at the very high values, we are essentially finding the majority label of the data points and assigning that label to this new point we need to classify. This is not a particularly useful way of classifying data and doesn't really use any of the actual data point features we might want to use to make the decision.\n",
    "\n",
    "Since we see that there is a range of points that are possible, I will be selecting the default $k=5$ amount. It is worth noting when $k=[23, 25, 27]$ the accuracy is perfect on the validation set, but we also do not want to over fit our model to that. So I will use the default which still garners a very high accuracy.\n",
    "\n",
    "Below I will reconstruct the kNN model with the default. If I was interested in being optimal, I likely would have saved the best version above instead of recreating it. For kNNs the \"training\" step is just collecting the data, but for other machine learning models this sort of optimization would save a lot of time/space/compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = knn(weights='uniform')\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results ###\n",
    "\n",
    "Since this is a classification problem, I will examine the accuracy of the model overall, as well as look at the resulting confusion matrix to assess how well the model performs on the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Confusion matrix:\n",
      "[[18  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "acc = model.score(X_test, Y_test)\n",
    "print('Accuracy:', acc)\n",
    "print()\n",
    "\n",
    "cmatrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "print('Confusion matrix:')\n",
    "print(cmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs perfectly on the data set, meaning it is very good at separating the 3 iris classes. It is worth noting I have previously applied several machine learning algorithms to this data set that did not perform as perfectly.\n",
    "\n",
    "| model | accuracy |\n",
    "| ------- | ------- |\n",
    "| Naive Bayes | 0.96 |\n",
    "| SVM -- polynomial | 0.87 |\n",
    "| SVM -- RBF | 1.0 |\n",
    "| SVM -- linear | 0.98 |\n",
    "\n",
    "Thus we find that the kNN model performs better than the Naive Bayes, and several SVM variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Weighted Model ###\n",
    "\n",
    "As mentioned, kNN can also be implemented to weight the votes of neighbors instead of using majority vote. Thus, points that are further away contribute less to the label decision of a given data point. scikit-learn implements this weighting as inverse the distnace of the neighbor.\n",
    "\n",
    "Similar to the above example, I will use the validation set to test different $k$ values and then construct the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 : 0.9523809523809523\n",
      "5 : 0.9523809523809523\n",
      "7 : 0.9523809523809523\n",
      "9 : 0.9523809523809523\n",
      "11 : 0.9523809523809523\n",
      "13 : 0.9523809523809523\n",
      "15 : 0.9523809523809523\n",
      "17 : 0.9523809523809523\n",
      "19 : 0.9523809523809523\n",
      "21 : 0.9523809523809523\n",
      "23 : 0.9523809523809523\n",
      "25 : 0.9523809523809523\n",
      "27 : 0.9523809523809523\n",
      "29 : 0.9523809523809523\n",
      "31 : 0.9523809523809523\n",
      "33 : 0.9523809523809523\n",
      "35 : 0.9523809523809523\n",
      "37 : 1.0\n",
      "39 : 0.9523809523809523\n",
      "41 : 1.0\n",
      "43 : 0.9523809523809523\n",
      "45 : 0.9523809523809523\n",
      "47 : 0.9523809523809523\n",
      "49 : 0.9523809523809523\n",
      "51 : 0.9523809523809523\n",
      "53 : 0.9523809523809523\n",
      "55 : 0.9523809523809523\n",
      "57 : 0.9523809523809523\n",
      "59 : 0.9523809523809523\n",
      "61 : 0.9523809523809523\n",
      "63 : 0.9523809523809523\n",
      "65 : 0.9523809523809523\n",
      "67 : 0.9523809523809523\n",
      "69 : 0.9523809523809523\n",
      "71 : 0.9523809523809523\n",
      "73 : 0.9523809523809523\n",
      "75 : 0.9523809523809523\n",
      "77 : 0.9523809523809523\n",
      "79 : 0.9523809523809523\n",
      "81 : 0.9523809523809523\n",
      "83 : 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "k_vals = [i for i in range(3, 85, 2)]\n",
    "\n",
    "for k in k_vals:\n",
    "    model = knn(n_neighbors=k, weights='distance')\n",
    "    model.fit(X_train, Y_train)\n",
    "    acc = model.score(X_val, Y_val)\n",
    "    print(k, ':', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, because we use weighting, we see that all values return a high accuracy. This time, the issue of the decreasing accuracy is not present because those points that are further away are weighted much less and so only those that are much closer will be weighted highly and contribute most to the label. Again, for arbitray easiness, I'll construct the model using the default $k=5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "[[18  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 14]]\n"
     ]
    }
   ],
   "source": [
    "model = knn(weights='distance')\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "acc = model.score(X_test, Y_test)\n",
    "print('Accuracy:', acc)\n",
    "print()\n",
    "cmatrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "print(cmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis #####\n",
    "\n",
    "In this case, the accuracy is the same as in the uniform kNN case. However, in the case where class distribution is highly skewed, the distance variant can perform much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
