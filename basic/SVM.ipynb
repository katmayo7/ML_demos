{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine #\n",
    "\n",
    "SVMs are a class of supervised machine learning algorithms. SVMs can be seen as similar to linear regression in that it is separating two classes, however, instead of measuring the separator against all data points, it will only use some of them. There's also a support vector clustering algorithm for unsupervised learning.\n",
    "\n",
    "The SVM algorithm identifies a hyperplane (line if just in 2D space) separating the classes that maximizes the distance between the hyperplane and the nearest points. This space between the hyperplane and nearest points is referred to as the *margin* and the nearest points are referred to as the *supports*. \n",
    "\n",
    "Given data points $x = (x_1, \\dotsc, x_n)$ and labels $y = (y_1, \\dotsc, y_n)$\n",
    "\n",
    "* Hyperplane: $w^Tx - b = 0$\n",
    "* Distance to supports: $d_i = \\frac{w^T x_i + b}{||w||}$\n",
    "* Linear classifier:\n",
    "    * $\\hat{y} = 1$ if $w^Tx + b \\geq 0$\n",
    "    * $\\hat{y} = 0$ if $w^Tx + b < 0$\n",
    "\n",
    "The algorithm may have a hard or soft margin, depending on whether the hyperplane is required to separate the data without misclassifications or allows misclassification, but with a penalty (usually hinge loss).\n",
    "\n",
    "##### Kernels ##### \n",
    "\n",
    "SVMs can be used to classify non-linear data using the kernel trick to map inputs into higher-dimensional space where the data may be separable.\n",
    "\n",
    "Popular kernels include:\n",
    "\n",
    "* linear\n",
    "* polynomial \n",
    "* gaussian RBF\n",
    "* sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "\n",
    "from sklearn import datasets, model_selection, pipeline, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data ###\n",
    "\n",
    "Import the built-in breast cancer data set and split into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X, Y, train_size=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Linear Model ###\n",
    "\n",
    "Create an SVM classification model with a linear kernal. Note that SVM works best with standardized mean and variance. I have integrated this data processing into the model creation using scikit-learn's Pipeline function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Linear Model ###\n",
    "\n",
    "The SVM model does a good job of classifying the malignant and benign cases, as evidenced by a high accuracy on the testing set and high precision and recall. In particular, the latters two measures show the model is correctly labeling positive cases as positive without much erronous positive labeling.\n",
    "\n",
    "##### For comparison:##### \n",
    "This data set was also demonstrated when implementing logistic regression with the following metrics:\n",
    "\n",
    "* Accuracy: 0.68\n",
    "* Precision: 0.64\n",
    "* Recall: 1\n",
    "* F1-score: 0.78\n",
    "\n",
    "Therefore, it can be seen that the SVM does a better job of classifying this data set than logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.992462311557789\n",
      "Testing set accuracy: 0.9532163742690059\n",
      "\n",
      "Precision: 0.9433962264150944\n",
      "Recall: 0.9803921568627451\n",
      "F1 score: 0.9615384615384616\n",
      "\n",
      "AUC: 0.9467178175618074\n"
     ]
    }
   ],
   "source": [
    "train_acc = model.score(X_train, Y_train)\n",
    "print('Training set accuracy:', train_acc)\n",
    "test_acc = model.score(X_test, Y_test)\n",
    "print('Testing set accuracy:', test_acc)\n",
    "print()\n",
    "\n",
    "prec = metrics.precision_score(Y_test, Y_pred)\n",
    "print('Precision:', prec)\n",
    "recall = metrics.recall_score(Y_test, Y_pred)\n",
    "print('Recall:', recall)\n",
    "f1 = metrics.f1_score(Y_test, Y_pred)\n",
    "print('F1 score:', f1)\n",
    "print()\n",
    "auc = metrics.roc_auc_score(Y_test, Y_pred)\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linearly Separable Data Example ##\n",
    "\n",
    "As mentioned above, SVMs can be used on data that is not linearly separable by using the kernel trick. In this example, I'll compare outcomes under two possible kernels: polynomial and RBF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Set ###\n",
    "\n",
    "Import the built-in Iris data set and divide into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X, Y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model: Polynomial Kernel ###\n",
    "\n",
    "Create SVM model using a polynomial kernel. The polynomial function requires the degree. In this case, selecting the degree is like selecting a hyperparameter value. Therefore, I will create a subset of the training data to be a validation set for testing the degree that should be used.\n",
    "\n",
    "Polynomial kernel:\n",
    "<center>$K(x, y) = (x^T y + c)^d$</center>\n",
    "\n",
    "Where $K$ is the kernel transforming data points $(x, y)$ with degree $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 2 Accuracy: 0.8571428571428571\n",
      "Degree: 3 Accuracy: 0.8571428571428571\n",
      "Degree: 4 Accuracy: 0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "# make validation set\n",
    "X_train,X_val,Y_train,Y_val = model_selection.train_test_split(X_train, Y_train, train_size=0.8)\n",
    "\n",
    "# degrees to test\n",
    "degrees = [2, 3, 4]\n",
    "\n",
    "for d in degrees:\n",
    "    model = pipeline.make_pipeline(StandardScaler(), SVC(kernel='poly', degree=d))\n",
    "    model.fit(X_train, Y_train)\n",
    "    # test accuracy on the hold out validation set\n",
    "    acc = model.score(X_val, Y_val)\n",
    "    print('Degree:', d, 'Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the randomness of the training and validation set groups can play a role here. Therefore, the above code block was run several times which resulted in three cases:\n",
    "\n",
    "* all degrees had same accuracy\n",
    "* degree 2 and 3 had the same accuracy that was better than degree 3\n",
    "* degree 3 had the best accuracy\n",
    "\n",
    "Based on these results, I will select to run with degree 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.make_pipeline(StandardScaler(), SVC(kernel='poly', degree=3))\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Model: Polynomial Kernel ###\n",
    "\n",
    "Analyzing the results using the accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8666666666666667\n",
      "\n",
      "Confusion matrix:\n",
      "[[16  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  6  9]]\n"
     ]
    }
   ],
   "source": [
    "acc = model.score(X_test, Y_test)\n",
    "print('Accuracy:', acc)\n",
    "print()\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "print('Confusion matrix:')\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show a decent accuracy. \n",
    "\n",
    "However, looking at the confusion matrix, it seems the model performs well with the first class but less well for the second and third classes. In particular, misclassifying some of the third class as members of the second. \n",
    "\n",
    "*Note:* in this data set, the first class is linearly separable from the other two, but the second and third are not. Therefore these issues make sense.\n",
    "\n",
    "| | predict 0 | predict 1 | predict 2|\n",
    "|-------| ------- | ------- | -------|\n",
    "| actual 0 | 16 | 0 | 0 |\n",
    "| actual 1 | 0 | 14 | 0 |\n",
    "| actual 2 | 0 | 6 | 9 |\n",
    "\n",
    "Caluating precision, recall, and F1 score as:\n",
    "\n",
    "* Precision: $\\frac{TP}{TP + FP}$\n",
    "* Recall: $\\frac{TP}{TP + FN}$\n",
    "* F1 score: $2 \\times \\frac{precision \\times recall}{precision + recall}$\n",
    "\n",
    "| Class | Precision | Recall | F1 score|\n",
    "| ------- | ------- | -------- | ------- |\n",
    "| 0 | 1.0 | 1.0 | 1.0 |\n",
    "| 1 | 0.7 | 1.0 | 0.82 |\n",
    "| 2 | 1.0 | 0.6 | 0.75 |\n",
    "\n",
    "These results show that the precision with labeling class 1 comes at the expense of missing some positive cases. In particular, as noted below it's an issue of distinguishing class 1 from class 2.\n",
    "\n",
    "When compared to the Naive Bayes model, I find that the SVM class performs the same for class 0, but worse for classes 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model: RBF Kernel ###\n",
    "\n",
    "Create an SVM model with the RBF Kernel, defined as:\n",
    "\n",
    "<center>$K(x, x') = exp(-\\frac{||x-x'||^2}{2\\sigma^2})$</center>\n",
    "\n",
    "where $||x-x'||^2$ is the squared Euclidean distnace between two data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer need a separate training and validation data set\n",
    "import numpy as np\n",
    "\n",
    "np.concatenate((X_train, X_val))\n",
    "np.concatenate((Y_train, Y_val))\n",
    "\n",
    "# construct model\n",
    "model = pipeline.make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Output: RBF Kernel ###\n",
    "\n",
    "Calculate the accuracy of the model and print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "[[15  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "acc = model.score(X_test, Y_test)\n",
    "print('Accuracy:', acc)\n",
    "print()\n",
    "\n",
    "conf_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on this model is perfect, therefore the confusion matrix also shows perfect precision, recall, and f1 score.\n",
    "\n",
    "Therefore, in this instance, the RBF kernel trick is better than the polynomial at making the data linearly separable and in turn, possible to perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Linear Model ###\n",
    "\n",
    "For reference, run a linear kernel SVM on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n",
      "\n",
      "[[15  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  1 11]]\n"
     ]
    }
   ],
   "source": [
    "# construct model using the same data set as in the RBF example.\n",
    "model = pipeline.make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "acc = model.score(X_test, Y_test)\n",
    "print('Accuracy:', acc)\n",
    "print()\n",
    "conf_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:\n",
    "\n",
    "| | predict 0 | predict 1 | predict 2|\n",
    "| ------- | ------- | ------- | -------- |\n",
    "| actual 0 | 15 | 0 | 0 |\n",
    "| actual 1 | 0 | 18 | 0 |\n",
    "| actual 2 | 0 | 1 | 11 |\n",
    "\n",
    "Recall as mentioned, class 0 can be linearly separated from the other 2 classes. We see that in the results here. In fact, we also see a simple linear model still distinguishes between class 1 and 2 better than the polynomial. However, the RBF function does the best still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
