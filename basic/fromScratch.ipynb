{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of from-scratch implementations of various basic machine learning algorithms and metrics for evaluating algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "# for testing\n",
    "from sklearn import datasets, model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics #\n",
    "\n",
    "Implementating various useful metrics for evaluating machine learning models. Should include metrics for classification (binary and multilabel) and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry i,j is the true label i labeled j\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    # get class labels in this data set\n",
    "    unique = list(set(y_true))\n",
    "    unique.sort()\n",
    "    num_classes = len(unique)\n",
    "    \n",
    "    # create and fill out confusion matrix\n",
    "    cmatrix = np.zeros((num_classes, num_classes))\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        true = unique.index(y_true[i])\n",
    "        pred = unique.index(y_pred[i])\n",
    "        cmatrix[true][pred] += 1\n",
    "    \n",
    "    return cmatrix\n",
    "\n",
    "# returns true positives for each class\n",
    "# true positive: actual and predicted this class\n",
    "def get_true_positives(y_true, y_pred, cmatrix=None):\n",
    "    tp = []\n",
    "    \n",
    "    if cmatrix is not None:\n",
    "        tp = np.diagonal(cmatrix)\n",
    "        \n",
    "    else:\n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        \n",
    "        # get list of classes \n",
    "        unique = list(set(y_true))\n",
    "        unique.sort()\n",
    "        \n",
    "        for u in unique:\n",
    "            tmp = np.sum(np.logical_and(y_true == u, y_pred == u))\n",
    "            tp.append(tmp)\n",
    "    \n",
    "    # in binary case, treat first label as the positive\n",
    "    if len(tp) == 2:\n",
    "        return tp[0]\n",
    "                \n",
    "    return tp\n",
    "\n",
    "# return true negatives for each class\n",
    "# true negative: actual and predicted are not of this class\n",
    "def get_true_negatives(y_true, y_pred, cmatrix=None):\n",
    "    tn = []\n",
    "    \n",
    "    if cmatrix is not None:\n",
    "        # get false negatives and false positives for each\n",
    "        # would double count true positives though, so arbitrarily subtract from one\n",
    "        fn = np.sum(cmatrix, axis=1)\n",
    "        fp = np.sum(cmatrix, axis=0) - np.diagonal(cmatrix)\n",
    "        \n",
    "        # get the total in the entire matrix\n",
    "        total = np.sum(fn)\n",
    "        \n",
    "        tn = total - (fn + fp)\n",
    "            \n",
    "    else:\n",
    "        unique = list(set(y_true))\n",
    "        unique.sort()\n",
    "        \n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        \n",
    "        for u in unique:\n",
    "            tmp = np.sum(np.logical_and(y_true != u, y_pred != u))\n",
    "            tn.append(tmp)\n",
    "    \n",
    "    # in binary case, treat second label as negative\n",
    "    if len(tn) == 2:\n",
    "        return tn[1]\n",
    "    \n",
    "    return tn\n",
    "\n",
    "# returns false positives for each class\n",
    "# false positive: predicted this class, not actually this class\n",
    "def get_false_positives(y_true, y_pred, cmatrix=None):\n",
    "    fp = []\n",
    "    \n",
    "    if cmatrix is not None:\n",
    "        fp = np.sum(cmatrix, axis=0) - np.diagonal(cmatrix)\n",
    "        \n",
    "    else:\n",
    "        unique = list(set(y_true))\n",
    "        unique.sort()\n",
    "        \n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "                \n",
    "        for u in unique:\n",
    "            tmp = np.sum(np.logical_and(y_true != u, y_pred == u))\n",
    "            fp.append(tmp)\n",
    "    \n",
    "    # in binary case, treat first label as positive\n",
    "    if len(fp) == 2:\n",
    "        return fp[0]\n",
    "    \n",
    "    return fp\n",
    "\n",
    "# returns false negatives for each class\n",
    "# false negative: actual this class, not predicted this class\n",
    "def get_false_negatives(y_true, y_pred, cmatrix=None):\n",
    "    fn = []\n",
    "    \n",
    "    if cmatrix is not None:\n",
    "        fn = np.sum(cmatrix, axis=1) - np.diagonal(cmatrix)\n",
    "    \n",
    "    else:\n",
    "        unique = list(set(y_true))\n",
    "        unique.sort()\n",
    "        \n",
    "        y_true = np.asarray(y_true)\n",
    "        y_pred = np.asarray(y_pred)\n",
    "        \n",
    "        for u in unique:\n",
    "            tmp = np.sum(np.logical_and(y_true == u, y_pred != u))\n",
    "            fn.append(tmp)\n",
    "    \n",
    "    # in binary case, treat second label as negative\n",
    "    if len(fn) == 2:\n",
    "        return fn[1]\n",
    "    \n",
    "    return fn\n",
    "\n",
    "# TP / all\n",
    "def accuracy(y_true, y_pred, cmatrix=None):\n",
    "    tps = np.asarray(get_true_positives(y_true, y_pred, cmatrix))\n",
    "    total = len(y_true)\n",
    "    \n",
    "    return np.sum(tps)/total\n",
    "    \n",
    "# returns for each class\n",
    "# TP / (TP + FN)\n",
    "def recall(y_true, y_pred, cmatrix=None):\n",
    "    tps = np.asarray(get_true_positives(y_true, y_pred, cmatrix))\n",
    "    fns = np.asarray(get_false_negatives(y_true, y_pred, cmatrix))\n",
    "    \n",
    "    recall = tps / (tps+fns)\n",
    "    \n",
    "    return recall\n",
    "\n",
    "# returns for each class\n",
    "# TP / (TP + FP)\n",
    "def precision(y_true, y_pred, cmatrix=None):\n",
    "    tps = np.asarray(get_true_positives(y_true, y_pred, cmatrix))\n",
    "    fps = np.asarray(get_false_positives(y_true, y_pred, cmatrix))\n",
    "    \n",
    "    prec = tps / (tps + fps)\n",
    "    \n",
    "    return prec\n",
    "\n",
    "# returns for each class\n",
    "# 2 * (precision * recall / precision + recall)\n",
    "def f1_score(y_true, y_pred, cmatrix=None):\n",
    "    prec = precision(y_true, y_pred, cmatrix)\n",
    "    rec = recall(y_true, y_pred, cmatrix)\n",
    "    \n",
    "    f1 = 2*((prec*rec)/(prec+rec))\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# calculate root mean squared error for regression problems\n",
    "def calc_rms(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    tmp = np.square(y_pred - y_true)\n",
    "    tmp = np.sum(tmp)/len(y_pred)\n",
    "    \n",
    "    return tmp**(1/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Methods #\n",
    "\n",
    "Implement from scratch various sampling methods. Generally oversampling, undersampling, and then getting a training and testing subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample with replacement from minority classes and add to data set so all classes equally represented\n",
    "def oversample(X, Y):\n",
    "    labels = list(set(Y))\n",
    "    label_counts = []\n",
    "    \n",
    "    # determine which classes are the majority class\n",
    "    max_label_val = 0\n",
    "    max_labels = []\n",
    "    \n",
    "    for l in labels:\n",
    "        tmp = np.sum(Y == l)\n",
    "        label_counts.append(tmp)\n",
    "        \n",
    "        if tmp > max_label_val:\n",
    "            max_labels = [l]\n",
    "            max_label_val = tmp\n",
    "        elif tmp == max_label_val:\n",
    "            max_labels.append(l)\n",
    "    \n",
    "    # oversample from the minority classes\n",
    "    for l in range(len(labels)):\n",
    "        if labels[l] not in max_labels:\n",
    "            sample_num = max_label_val - label_counts[l] \n",
    "            \n",
    "            # get the indices from the actual data set that have this label\n",
    "            tmp_data = np.where(Y == labels[l])[0]\n",
    "            to_add = np.random.choice(tmp_data, sample_num)\n",
    "            \n",
    "            for ta in to_add:\n",
    "                tmp = np.reshape(X[ta], (1, len(X[ta])))\n",
    "                X = np.append(X, tmp, axis=0)\n",
    "                Y = np.append(Y, Y[ta])\n",
    "                \n",
    "    return X,Y\n",
    "\n",
    "# remove data points from the majority classes so that all classes equally represented\n",
    "def undersample(X, Y):\n",
    "    labels = list(set(Y))\n",
    "    label_counts = []\n",
    "    \n",
    "    # determine which classes are the minority class\n",
    "    min_labels_val = len(X) + 1\n",
    "    min_labels = []\n",
    "    \n",
    "    for l in labels:\n",
    "        tmp = np.sum(Y == l)\n",
    "        label_counts.append(tmp)\n",
    "        \n",
    "        if tmp < min_labels_val:\n",
    "            min_labels = [l]\n",
    "            min_label_val = tmp\n",
    "        elif tmp == min_labels_val:\n",
    "            min_labels.append(l)\n",
    "    \n",
    "    # remove samples from majority classes\n",
    "    for l in range(len(labels)):\n",
    "        if labels[l] not in min_labels:\n",
    "            remove_num = label_counts[l] - min_label_val\n",
    "            \n",
    "            # get indices from actual data set with this label\n",
    "            tmp_data = np.where(Y == l)[0]\n",
    "            to_remove = np.random.choice(tmp_data, remove_num, replace=False)\n",
    "            \n",
    "            for tr in to_remove:\n",
    "                X = np.delete(X, tr, axis=0)\n",
    "                Y = np.delete(Y, tr)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "# train_per is what percent of data set should be the training data\n",
    "def split_train_test(X, Y, train_per=0.5):\n",
    "    num_train = int(train_per*len(X))\n",
    "    indices = list(np.arange(len(Y)))\n",
    "        \n",
    "    train_ind = rnd.sample(indices, num_train)\n",
    "    X_train = np.take(X, train_ind, axis=0)\n",
    "    Y_train = np.take(Y, train_ind)\n",
    "    \n",
    "    X_test = np.delete(X, train_ind, axis=0)\n",
    "    Y_test = np.delete(Y, train_ind)\n",
    "    \n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors Implementation #\n",
    "\n",
    "Implementing from scratch the k-nearest neighbors algorithm. Which classifies a given data point as the majority vote label of the k-training data points nearest to the test data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN:\n",
    "    \n",
    "    # weights: {uniform, distance};\n",
    "    # euclidean: p = 2; manhattan: p = 1\n",
    "    def __init__(self, k, weights='uniform', p=2):\n",
    "        self.k = k\n",
    "        self.weights = weights\n",
    "        #self.distance = distance\n",
    "        self.p = p\n",
    "        \n",
    "        self.data = None\n",
    "        self.labels = None\n",
    "    \n",
    "    # training is simply taking in the data set that will be used\n",
    "    def train(self, X, Y):\n",
    "        self.data = np.asarray(X)\n",
    "        self.labels = Y\n",
    "    \n",
    "    # testing returns the labels for a list of testing data points\n",
    "    def test(self, X):\n",
    "        predicted_labels = []\n",
    "        \n",
    "        for x in X:\n",
    "            dists = np.power(np.subtract(self.data, x), self.p)\n",
    "            dists = np.power(np.sum(dists, axis=1), 1/self.p)\n",
    "        \n",
    "            # order by distances\n",
    "            sorted_dists = np.argsort(dists)\n",
    "            \n",
    "            # plurality vote by k-nearest training data points\n",
    "            labels = [self.labels[sorted_dists[i]] for i in range(self.k)]\n",
    "            if self.weights == 'uniform':\n",
    "                pred = max(set(labels), key=labels.count)\n",
    "            else:\n",
    "                weights = {}\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i] not in weights:\n",
    "                        weights[labels[i]] = 0\n",
    "                    weights[labels[i]] += 1/(i+1)\n",
    "                \n",
    "                pred = max(weights, key=weights.get)\n",
    "            \n",
    "            predicted_labels.append(pred)\n",
    "        \n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test implementation ####\n",
    "\n",
    "Test the from scratch implementation by comparing the accuracy of the method versus the scikit-learn implementation on the built-in Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My confusion matrix:\n",
      "[[18.  0.  0.]\n",
      " [ 0.  9.  1.]\n",
      " [ 0.  0. 17.]]\n",
      "\n",
      "Sklearn confusion matrix:\n",
      "[[18.  0.  0.]\n",
      " [ 0.  9.  1.]\n",
      " [ 0.  0. 17.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iris data set\n",
    "data = datasets.load_iris()\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "\n",
    "# separate into training and testing set\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X, Y, train_size=0.7, random_state=123)\n",
    "\n",
    "model = kNN(5)\n",
    "model.train(X_train, Y_train)\n",
    "Y_preds = model.test(X_test)\n",
    "\n",
    "cmatrix = confusion_matrix(Y_test, Y_preds)\n",
    "print('My confusion matrix:')\n",
    "print(cmatrix)\n",
    "print()\n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "sk_model = neighbors.KNeighborsClassifier()\n",
    "sk_model.fit(X_train, Y_train)\n",
    "preds = sk_model.predict(X_test)\n",
    "\n",
    "sk_cmatrix = confusion_matrix(Y_test, preds)\n",
    "print('Sklearn confusion matrix:')\n",
    "print(sk_cmatrix)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network #\n",
    "\n",
    "Implementation of some of the aspects of a neural network including activation functions and the feed forward section. This was created following the tutorial: https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network with random parameters between 0 and 1\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    params = {}\n",
    "    \n",
    "    for l in range(1, len(layer_dims)):\n",
    "        # weights should be in the shape: layers this dimension x layers last dimension\n",
    "        # pull from standard normal distribution\n",
    "        params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])\n",
    "        params['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions ####\n",
    "\n",
    "Implementation of sigmoid and ReLU acitvation functions that introduce non-linearity to the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    \n",
    "    return A,Z\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    \n",
    "    return A,Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed forward pass ####\n",
    "\n",
    "Implementation of the forward pass of the NN, which takes inputs from a previous layer and calculates: $z = W^Tx + b$.\n",
    "\n",
    "After calculating that, it applies an activation function g(z). Each run needs to store the variables computed during the calculation so that they can be called on during the backpropagation part.\n",
    "\n",
    "It will do this for every layer in the network. After the last layer, it will pass through the final forward with non-linear, in this case a sigmoid, which will provide the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_helper(X, W, b, activation):\n",
    "    Z = np.dot(W, X) + b\n",
    "    cache1 = (X, W, b)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        Z,cache2 = sigmoid(Z)\n",
    "    else:\n",
    "        Z,cache2 = relu(Z)\n",
    "    \n",
    "    return Z,(cache1, cache2)\n",
    "\n",
    "def forward_pass(X, params, hidden_layer_activation='relu'):\n",
    "    # save info for backprop\n",
    "    caches = []\n",
    "    # parameters stores bias+weights, so need to divide length by 2\n",
    "    num_layers = len(params)//2\n",
    "    \n",
    "    for l in range(1, num_layers):\n",
    "        X_prev = X\n",
    "        X,cache = forward_helper(X_prev, params['W'+str(l)], params['b'+str(l)], hidden_layer_activation)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    X,cache = forward_helper(X, params['W'+str(l)], params['b'+str(l), 'sigmoid'])\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return X,caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy cost ####\n",
    "\n",
    "Implementing binary cross-entropy loss for cost, which uses log-likelihood to estimate its error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y):\n",
    "    cost = -(1/y.shape[1]) * np.sum(np.multiply(y, np.log(X)) + np.multiply(1-y, np.log(1-X)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back-propagation ####\n",
    "\n",
    "Backprop allows the information to flow backwards in the network so weights can be updated based on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(dX, Z):\n",
    "    A,Z = sigmoid(Z)\n",
    "    dZ = dX*A*(1-A)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def relu_gradient(dX, Z):\n",
    "    A,Z = relu(Z)\n",
    "    dZ = np.multiply(dX, np.int64(A>0))\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def backward_helper(dX, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_gradient(dX, activation_cache)\n",
    "    else:\n",
    "        dZ = relu_gradient(dX, activation_cache)\n",
    "    \n",
    "    # backward for forward pass\n",
    "    A_prev,W,b = linear_cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def backward_pass(X, y, caches, hidden_layer_activation='relu'):\n",
    "    y = y.reshape(X.shape)\n",
    "    L = len(caches)\n",
    "    grads = {}\n",
    "    \n",
    "    # have to go backward through the network\n",
    "    dX = np.divide(X-y, np.multiply(X, 1-X))\n",
    "    grads['dX' + str(L-1)],grads['dW'+str(L)],grads['db'+str(L)] = backward_helper(dX, caches[L-1], 'sigmoid')\n",
    "    \n",
    "    for l in range(L-1, 0, -1):\n",
    "        curr_cache = caches[l-1]\n",
    "        grads['dX'+str(l-1)],grads['dW'+str(l)],grads['db'+str(l)] = backward_helper(grads['dX'+str(l)], curr_cache, hidden_layers_activation)\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_params(params, grads, learning_rate):\n",
    "    L = len(params)//2\n",
    "    \n",
    "    for l in range(1, L+1):\n",
    "        params['W'+str(l)] = params['W'+str(l)] - learning_rate*grads['dW'+str(l)]\n",
    "        params['b'+str(l)] = params['b'+str(l)] - learning_rate*grads['db'+str(l)]\n",
    "    \n",
    "    return params\n",
    "                                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will all be put together to create a network. The network is trained by over many iterations, calling: forward pass, cost, backward pass, update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
